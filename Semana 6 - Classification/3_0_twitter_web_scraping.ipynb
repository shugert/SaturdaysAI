{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction Into APIs\n",
    "\n",
    "\n",
    "<a href=\"https://docs.google.com/presentation/d/1BVAnUxU2tYaM7an8n0fosuVWT3viIJS8WQDS16DEBwQ/edit\">\n",
    "     <img src=\"http://www.deltanalytics.org/uploads/2/6/1/4/26140521/screen-shot-2019-01-06-at-3-37-12-pm_orig.png\" width=\"500\" height=\"400\">\n",
    "</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "def pull_tweets (query, maxTweets = 1000, tweetsPerQuery = 100, max_id = -1, sinceId = None):\n",
    "    '''\n",
    "    Finds tweets (Comment, Date, Favorites, User) for a query string.\n",
    "    Twitter API limit per query is 100. Combines these queries. \n",
    "    '''\n",
    "    \n",
    "    # Fill with your own app details\n",
    "    API_KEY = \"wCl2jflXpyWmYDM22iKFsaiS2\"\n",
    "    API_SEC = \"f3DkCao13uCfA58bSQXahsDVNF5qzNztrgt3wB2RDDAV8zyXvT\"\n",
    "    \n",
    "    # connect to Twitter using authentication\n",
    "    auth = tweepy.AppAuthHandler(API_KEY, API_SEC)\n",
    "    # wait_on_rate_limit means that if the API limit is hit, \n",
    "    #   the pulls will wait until more calls are available\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "    \n",
    "    # Pull comments from Twitter\n",
    "    # See https://developer.twitter.com/en/docs/tweets/timelines/guides/working-with-timelines\n",
    "    tweetCount = 0\n",
    "    data = pd.DataFrame() \n",
    "    \n",
    "    while tweetCount < maxTweets:\n",
    "        if (max_id <= 0):\n",
    "            new_tweets = api.search(q=query, count=tweetsPerQuery, \n",
    "                                    since_id=sinceId)\n",
    "        else:\n",
    "            new_tweets = api.search(q=query, count=tweetsPerQuery,\n",
    "                                    max_id=str(max_id - 1), \n",
    "                                    since_id=sinceId)\n",
    "        if not new_tweets:\n",
    "            print(\"No more tweets found\")\n",
    "            break\n",
    "    \n",
    "        tweetCount += len(new_tweets)\n",
    "        print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "        max_id = new_tweets[-1].id\n",
    "                    \n",
    "        ## Create a dataset from the downloaded tweets\n",
    "        new_data = pd.DataFrame([{\n",
    "                'Date': tweet.created_at,\n",
    "                'Comments': tweet.text, \n",
    "                'User': tweet.user.name, \n",
    "                'Favorites': tweet.favorite_count} \n",
    "                for tweet in new_tweets])\n",
    "          \n",
    "        data = data.append(new_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 100 tweets\n",
      "Downloaded 200 tweets\n",
      "Downloaded 300 tweets\n",
      "Downloaded 400 tweets\n",
      "Downloaded 500 tweets\n",
      "Downloaded 600 tweets\n",
      "Downloaded 700 tweets\n",
      "Downloaded 800 tweets\n",
      "Downloaded 900 tweets\n",
      "Downloaded 1000 tweets\n",
      "Downloaded 1100 tweets\n",
      "Downloaded 1188 tweets\n",
      "Downloaded 1288 tweets\n",
      "Downloaded 1388 tweets\n",
      "Downloaded 1488 tweets\n",
      "Downloaded 1588 tweets\n",
      "Downloaded 1688 tweets\n",
      "Downloaded 1788 tweets\n",
      "Downloaded 1888 tweets\n",
      "Downloaded 1988 tweets\n",
      "Downloaded 2088 tweets\n",
      "Downloaded 2188 tweets\n",
      "Downloaded 2288 tweets\n",
      "Downloaded 2388 tweets\n",
      "Downloaded 2487 tweets\n",
      "Downloaded 2558 tweets\n",
      "No more tweets found\n"
     ]
    }
   ],
   "source": [
    "data = pull_tweets(\"microfinance\", maxTweets = 5000)\n",
    "\n",
    "# In real life you might have test data with pre-labeled sentiments. We will use a simple word net algorithm to classify for now.\n",
    "data['Polarity'] = [TextBlob(comment).polarity for comment in data.Comments]\n",
    "\n",
    "data.loc[data['Polarity'] > 0, 'Sentiment'] = 'positive'\n",
    "data.loc[data['Polarity'] < 0, 'Sentiment'] = 'negative'\n",
    "data.loc[data['Polarity'] == 0, 'Sentiment'] = 'neutral'\n",
    "\n",
    "#convert data to a csv\n",
    "data.to_csv(\"microfinance_tweets.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
